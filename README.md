# üîç Manus Monitor - Manhaj 2030 Real-Time Monitoring

**Real-time monitoring system for Manhaj 2030 Islamic Hadith NER Project**

[![Update Frequency](https://img.shields.io/badge/Updates-Every%201%20Minute-brightgreen)](https://github.com/ysu444/manus-monitor)
[![Version](https://img.shields.io/badge/Version-4.0.0-blue)](https://github.com/ysu444/manus-monitor)
[![Zero Cache](https://img.shields.io/badge/Cache-Zero%20(SHA--based)-orange)](https://raw.githubusercontent.com/ysu444/manus-monitor/main/manifest.json)

---

## üìã Overview

This repository provides **real-time monitoring data** for the Manhaj 2030 project, a state-of-the-art Islamic Hadith Named Entity Recognition (NER) system. Data is automatically updated **every minute** from the production server (45.32.106.139) and served via **SHA-based URLs** with **zero CDN caching** to ensure LLMs and consultants always access the latest information.

**Key Features:**
- ‚úÖ **Real-time updates** (every 1 minute via Cron)
- ‚úÖ **Zero CDN caching** (SHA-based URLs - v4.0 breakthrough!)
- ‚úÖ **Always fresh data** (< 60 seconds latency)
- ‚úÖ **Comprehensive metadata** (file sizes, timestamps, commit SHA)
- ‚úÖ **15 active agents** monitoring system health and project progress
- ‚úÖ **DHVS 2025 compliant** scientific closure tracking

---

## üñ•Ô∏è Hardware Specifications (Verified 2025-10-25)

### Physical Infrastructure

**CPU:**
- 2 √ó AMD EPYC 7543 (64 cores each = **128 logical cores total**)
- Base clock: 2.8 GHz, Boost up to 3.7 GHz
- 256 MB L3 cache

**RAM:**
- **926 GB DDR4 ECC** (3200 MHz)
- Sufficient for large-scale NER training and inference

**Storage:**
- **1.4 TB NVMe SSD** (>5.5 GB/s read speed) - Primary workspace
- **1.5 TB SATA** - Archive storage
- Total: ~2.9 TB

**GPU:**
- **16 √ó NVIDIA A16-16Q vGPU** (Ampere architecture)
- **16 GB VRAM per GPU** (256 GB total)
- ~2,560 CUDA cores per GPU (~40,960 total)
- 80 Tensor Cores per GPU (1,280 total)
- **95-98% physical GPU performance** (vGPU vs bare metal)
- Supports multi-GPU training (DDP with NCCL)

**Network:**
- IP: **45.32.106.139** (Vultr production server)
- High-bandwidth connection for data transfer

---

### Software Stack

**Operating System:**
- Ubuntu 22.04 LTS 64-bit

**NVIDIA Stack:**
- Driver: 550.90.07
- CUDA: v12.2
- cuDNN: v8.9

**Deep Learning Framework:**
- PyTorch: 2.3 with CUDA support
- Transformers: v4.44 (Hugging Face)
- DDP Backend: NCCL v2.18 (supports 2-16 GPU parallel training)

**Python Environment:**
- Python: 3.10 (venv_manhaj)
- Key packages: torch, transformers, datasets, accelerate

---

### Performance Metrics

**Training Performance:**
- Batch size: **512 per GPU** (NER tasks)
- Step time: **~2.3s per step** (single GPU)
- DDP scaling efficiency:
  - 4 GPU ‚Üí **3.8√ó speedup** (95% efficiency)
  - 8 GPU ‚Üí **6.5√ó speedup** (81% efficiency)

**Monitoring:**
- Update frequency: **Every 1 minute** via Cron
- Data delivery: **SHA-based URLs** (zero cache!)
- Uptime: 99.9%+

---

### üí° Strategic Decision (2025-10-25)

**‚úÖ No dedicated server needed** ‚Äî Current vGPU infrastructure is fully adequate

**Rationale:**
- **Cost savings:** $12,900/month saved vs dedicated GPU server
- **Performance:** 95-98% vs dedicated (2-5% difference negligible for research)
- **Capacity:** Sufficient for all Phase F4-F10 and G-series work
- **Scalability:** 16 GPUs support up to 16-way parallel training

**When to reconsider:**
- Training LLMs with **>7B parameters**
- Requiring **H100 architecture** (next-gen Hopper GPUs)
- Need for **>16 GB VRAM per GPU** (current: 16 GB A16)

**Current verdict:** Infrastructure is optimal for Manhaj 2030 NER project requirements.

---

## üöÄ Quick Start for LLMs (Claude, GPT, etc.)

### ‚ö° NEW: Zero-Cache Access (v4.0)

**The Problem We Solved:**
- GitHub Raw URLs use CDN caching (5-60 minutes delay)
- LLMs were reading stale data

**The Solution:**
- **SHA-based URLs** - Each commit has a unique SHA
- **Zero caching** - Every URL is unique, no CDN cache!
- **Always fresh** - Data updated every minute with new SHA

---

### üìñ How to Access Real-Time Data

**Step 1: Read the manifest**

```
https://raw.githubusercontent.com/ysu444/manus-monitor/main/manifest.json
```

**What you get:**
```json
{
  "generated_at": "2025-10-25T09:28:49Z",
  "commit_sha": "b3397d2606182f3d19dd2263e5a899fc45ee12c7",
  "commit_sha_short": "b3397d2",
  "version": "4.0.0",
  
  "direct_links": {
    "raw_github_live": {
      "description": "Use these links for real-time data (zero cache)",
      "state": "https://raw.githubusercontent.com/.../b3397d2/manhaj2030/monitor/state.json",
      "agents_status": "https://raw.githubusercontent.com/.../b3397d2/manhaj2030/pmc/inbox/AGENTS_STATUS.json",
      ...
    }
  }
}
```

---

**Step 2: Extract the commit SHA**

```json
"commit_sha_short": "b3397d2"
```

---

**Step 3: Use the SHA-based URLs from `direct_links.raw_github_live`**

```
state: https://raw.githubusercontent.com/ysu444/manus-monitor/b3397d2/manhaj2030/monitor/state.json
agents_status: https://raw.githubusercontent.com/ysu444/manus-monitor/b3397d2/manhaj2030/pmc/inbox/AGENTS_STATUS.json
pmc_events: https://raw.githubusercontent.com/ysu444/manus-monitor/b3397d2/manhaj2030/pmc/memory/events.jsonl
```

**Why this works:**
- ‚úÖ Each SHA is unique to a specific commit
- ‚úÖ New commit every minute = new SHA = new URL
- ‚úÖ GitHub cannot cache (every URL is different!)
- ‚úÖ You always get the latest data

---

### üéØ Example Usage

**Query:** "Check server health"

**LLM Process:**
```
1. Read: https://raw.githubusercontent.com/ysu444/manus-monitor/main/manifest.json
2. Extract: commit_sha = "b3397d2"
3. Read: https://raw.githubusercontent.com/.../b3397d2/manhaj2030/monitor/state.json
4. Analyze: Fresh data (timestamp: 09:20:18Z)
5. Report: System healthy, 338 events, last event: DDP_REFACTOR_V1_GLOO_SUCCESS
```

---

## üìä File Sizes and Reading Strategy

### Small Files (< 5 KB) - Read Fully ‚úÖ

| File | Size | Description | How to Access |
|:-----|:-----|:------------|:--------------|
| **manifest.json** | ~2 KB | Master index with SHA | Read from `main` branch first |
| **system-info.json** | ~500 B | CPU, Memory, Disk, Uptime | Use SHA-based URL from manifest |
| **pm2-status.json** | ~100 B | PM2 process status | Use SHA-based URL from manifest |
| **state.json** | ~1 KB | System state (GPU, Events, Agents) | Use SHA-based URL from manifest |
| **AGENTS_STATUS.json** | ~2 KB | Status of all 15 Agents | Use SHA-based URL from manifest |

**Strategy:** Read these files completely. They provide maximum insight with minimal token usage.

---

### Medium Files (5-50 KB) - Read Selectively ‚ö†Ô∏è

| File | Size | Description | Reading Strategy |
|:-----|:-----|:------------|:-----------------|
| **realtime_state.json** | ~50 KB | File monitoring state | Read first 100 lines or search for specific files |
| **events.jsonl** | ~30 KB | PMC event log (JSONL) | **Read last 50 lines only** (most recent events) |
| **scientific_closure_v1.md** | ~15 KB | Scientific closure report | Read summary sections first |

**Strategy:** 
- For **events.jsonl**: Always read **last 50 lines** (most recent events)
- For **realtime_state.json**: Search for specific files or read first 100 lines
- For **scientific_closure_v1.md**: Read executive summary and Gate status sections

---

### Large Files (> 50 KB) - Read Strategically üî¥

| File | Size | Description | Reading Strategy |
|:-----|:-----|:------------|:-----------------|
| **pmc_unified_latest.md** | ~100 KB | Comprehensive PMC report | **Read table of contents first**, then specific sections |

**Strategy:**
- Read **table of contents** or **first 200 lines**
- Identify relevant sections
- Read only those sections
- Avoid reading the entire file unless specifically requested

---

## üéØ Optimal Reading Patterns

### Pattern 1: Quick Health Check (< 5K tokens)

```
1. Read manifest.json (get latest SHA)
2. Read system-info.json (using SHA-based URL)
3. Read state.json (using SHA-based URL)
```

**Total:** ~3 KB | **Tokens:** ~1,500 | **Time:** < 5 seconds | **Freshness:** < 60 seconds

---

### Pattern 2: Project Progress Review (< 15K tokens)

```
1. Read manifest.json (get latest SHA)
2. Read state.json (current phase, events count)
3. Read AGENTS_STATUS.json (agent activity)
4. Read events.jsonl (LAST 50 LINES ONLY)
5. Read scientific_closure_v1.md (summary sections)
```

**Total:** ~15 KB | **Tokens:** ~7,500 | **Time:** < 15 seconds | **Freshness:** < 60 seconds

---

### Pattern 3: Deep Analysis (< 50K tokens)

```
1. Read manifest.json (get latest SHA)
2. Read state.json (system state)
3. Read scientific_closure_v1.md (full report)
4. Read pmc_unified_latest.md (TOC + relevant sections)
5. Read events.jsonl (last 100 lines)
```

**Total:** ~50 KB | **Tokens:** ~25,000 | **Time:** < 30 seconds | **Freshness:** < 60 seconds

---

## üìÅ File Descriptions

### System Monitoring

**manifest.json** - Master index (START HERE!)
- Current timestamp (Unix, ISO, Human-readable)
- Commit SHA (for SHA-based URLs)
- File metadata (size, last modified, exists)
- Direct links with SHA-based URLs (zero cache!)

**system-info.json** - System resources
- CPU usage (%)
- Memory (total, used, free, %)
- Disk (total, used, available, %)
- Uptime (seconds)

**pm2-status.json** - Process manager
- PM2 process list (if available)
- Process status, CPU, memory

---

### Project Monitoring

**state.json** - System state
- Timestamp
- Total events count
- Last event name and timestamp
- GPU metrics (count, utilization, memory)
- System metrics (CPU, memory, disk, overall status)
- Active agents list

**realtime_state.json** - File monitoring
- SHA256 checksums of monitored files
- File paths, timestamps, sizes
- Change detection

**AGENTS_STATUS.json** - Agent status
- All 15 agents with:
  - ID, name, role
  - Status (active/inactive)
  - Last refresh timestamp

---

### Project Data

**events.jsonl** - Event log
- JSONL format (one JSON object per line)
- Events from all 15 agents
- Timestamps, event types, metadata
- **‚ö†Ô∏è Large file:** Read last 50-100 lines only

**pmc_unified_latest.md** - Unified report
- Comprehensive project report
- Phase progress, Gate status
- Agent activities, system health
- **‚ö†Ô∏è Large file:** Read TOC first, then specific sections

**scientific_closure_v1.md** - Scientific closure
- DHVS 2025 compliance report
- Gate A-E status and metrics
- Model performance (F1, Precision, Recall, ECE, Brier)
- Recommendations and next steps

**training_metrics.json** - Training metrics (optional)
- Model training metrics
- May not always be present

---

## üö¶ Data Freshness Guidelines

| Age | Status | Action |
|:----|:-------|:-------|
| < 2 minutes | ‚úÖ Fresh | Proceed with analysis |
| 2-5 minutes | ‚ö†Ô∏è Acceptable | Check if Cron is running |
| > 5 minutes | ‚ùå Stale | Investigate server/Cron issues |

**How to check freshness:**
1. Read `manifest.json`
2. Compare `generated_at` with current time
3. If > 5 minutes, something is wrong

---

## üîß Technical Details

### Update Mechanism

**Cron Job:**
```bash
* * * * * /usr/local/bin/push_monitor_to_github.sh >> /var/log/manus-monitor-push.log 2>&1
```

**Update Process:**
1. Collect system info (CPU, Memory, Disk)
2. Collect PM2 status
3. Copy manhaj2030 files
4. Create Git commit #1 (data files)
5. Generate manifest.json with SHA from commit #1
6. Create Git commit #2 (manifest with SHA-based URLs)
7. Push to GitHub

**Result:**
- Commit #1 contains: state.json, events.jsonl, etc.
- Commit #2 contains: manifest.json with SHA pointing to Commit #1
- SHA-based URLs in manifest point to Commit #1 (where data lives)
- Zero caching because each URL is unique!

---

### File Mapping

| Source (Server) | Destination (GitHub) |
|:----------------|:---------------------|
| `/srv/manhaj2030/pmc/shared/collective_state.json` | `manhaj2030/monitor/state.json` |
| `/srv/manhaj2030/pmc/memory/AGENTS_STATUS.json` | `manhaj2030/pmc/inbox/AGENTS_STATUS.json` |
| `/srv/manhaj2030/pmc/memory/events.jsonl` | `manhaj2030/pmc/memory/events.jsonl` |
| `/srv/manhaj2030/pmc/unified/pmc_unified_latest.md` | `manhaj2030/pmc/unified/pmc_unified_latest.md` |
| `/srv/manhaj2030/reports/scientific_closure_v1.md` | `manhaj2030/reports/scientific_closure_v1.md` |

---

## üìû Contact & Roles

**Project Owner:** Abu Khalid (ÿ£ÿ®Ÿà ÿÆÿßŸÑÿØ)

**AI Consultants:**
- Claude (Anthropic) - Strategic advisor and code reviewer
- Manus Agents (1-15) - Automated monitoring and reporting

**Repository:** https://github.com/ysu444/manus-monitor

**Server:** 45.32.106.139 (Vultr)

---

## üìú License

This repository is for monitoring purposes only. All data is related to the Manhaj 2030 Islamic Hadith NER project.

---

## üéâ Version History

**v4.0.0 (2025-10-25)** - SHA-based URLs breakthrough!
- ‚úÖ Zero CDN caching
- ‚úÖ Always fresh data (< 60 seconds)
- ‚úÖ Two-commit strategy (data + manifest)
- ‚úÖ SHA-based URLs in manifest

**v3.0.0 (2025-10-24)** - Cache-busting improvements
- ‚úÖ Timestamp parameters
- ‚úÖ GitHub Pages optimization
- ‚ö†Ô∏è Still had 5-60 min caching issues

**v2.0.0 (2025-10-24)** - Initial GitHub Pages setup
- ‚úÖ Automated updates every 1 minute
- ‚úÖ Comprehensive file coverage
- ‚ö†Ô∏è CDN caching problems

---

**üöÄ Now with Zero Caching - Always Fresh Data! üéä**

